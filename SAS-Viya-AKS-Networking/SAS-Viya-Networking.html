<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>SAS-Viya-Networking.html</title>
<meta http-equiv="Content-Type" content="application/xhtml+xml;charset=utf-8"/>
<link rel="stylesheet" type="text/css" media="all" href="https://cdn.jsdelivr.net/npm/github-markdown-css/github-markdown.min.css"  />
<link rel="stylesheet" type="text/css" media="all" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/github.min.css"  />
<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
<style>
body {
  box-sizing: border-box;
  max-width: 740px;
  width: 100%;
  margin: 40px auto;
  padding: 0 10px;
}
</style>

<link rel='stylesheet' href='https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/default.min.css'>
<script src='https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/highlight.min.js'></script>
<script>
document.addEventListener('DOMContentLoaded', () => {
  document.body.classList.add('markdown-body');
  document.querySelectorAll('pre code').forEach((code) => {
    if (code.className != 'mermaid') {
      hljs.highlightBlock(code);
    }
  });
});
</script>

<script src='https://unpkg.com/mermaid@8.4.8/dist/mermaid.min.js'></script>
<script>
mermaid.initialize({
  theme: 'default',  // default, forest, dark, neutral
  startOnLoad: true
});
</script>

</head>

<body>

[WARNING] Could not parse YAML metadata at line 6 column 1: Expected start of line
<h1 id="introdution">Introdution</h1>
<blockquote>
<p>In essence, Kubernetes is the sum of all the bash scripts and best practices that most system administrators would cobble together over time, presented as a single system behind a declarative set of APIs.</p>
<p>– <cite>Kelsey Hightower</cite></p>
</blockquote>
<p>SAS Viya has adopted Kubernetes as the underlying container orchestration platform, opening up immense possibilities for all that wish to combine the industry-leading abilities of SAS in analytics, data science and AI, with a cloud-native deployment approach that aligns with current strategic roadmaps, not the least because of the potential benefits it brings to the table: integration with CI/CD for deployment and code, enabling hybrid cloud scenarios by supporting both on-premises and cloud deployments, multi-cloud support by default, and perhaps more importantly, a product development process that is more agile, with more releases, a faster incorporation of new features.</p>
<figure>
<img src="https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg" alt="" /><figcaption>Evolution of deployment strategies</figcaption>
</figure>
<p>If the move towards Kubernetes seems obvious today (especially considering that Kubernetes has become the industry standard in container orchestration, and is the underlying platform on which a lot of other extremely popular technologies are built on), someone coming from a SAS 9 background can find the target architecture somewhat hermetic: instead of a number of servers with a specific set of well-known components, SAS Viya depends on nodes, deployments, services, ingresses, and pods, plus many other concepts that can seem complex - and especially so in terms of the network aspects.</p>
<figure>
<img src="./sas-studio-window.png" alt="" /><figcaption>SAS Studio in SAS Viya</figcaption>
</figure>
<p>When using SAS Studio in SAS Viya, how do our requests get there? This is precisely the aspect we will address in this article: how do network requests work in a Kubernetes context, starting from a single pod and working our way out of the Kubernetes onion, peeling off the apparent complexity layer by layer - and once we are comfortably on the outside, trace our way back, now with a clearer understanding of how SAS Viya and Kubernetes work.</p>
<h1 id="kubernetes-an-overview-of-the-network-model">Kubernetes: an overview of the network model</h1>
<p>There is a lot of good information about Kubernetes networking, starting from <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">the official one</a>. In more than one way, the Kubernetes network model is a set of principles whose implementation is done by specific components (namely, Container Network Plugins, CNI).</p>
<p>It’s not our intention to go into the details of all the options, and the possibilities of using different approaches. Our environment will be based on</p>
<ul>
<li>A Kubernetes 1.23 cluster on Microsoft Azure (Azure Kubernetes Service).</li>
<li>SAS Viya 2022.1.3, deployed in a <code>sas-viya</code> namespace (configured to be the active Kubernetes context)</li>
<li>Use of the AKS default <code>kubenet</code> CNI.</li>
<li>Use of the NGINX Ingress controller</li>
</ul>
<figure>
<img src="https://documentation.sas.com/api/docsets/itopscon/v_015/content/images/aks-arch.svg?locale=en" alt="" /><figcaption>SAS Viya on Azure Kubernetes Service</figcaption>
</figure>
<p>Given that we are describing standard Kubernetes concepts, the vast majority of this article will apply to all Kubernetes and SAS Viya versions.</p>
<p><a href="https://docs.microsoft.com/en-us/azure/aks/concepts-network#kubenet-basic-networking">Kubenet</a> is specific to Microsoft Azure, and the network model can be s</p>
<ol type="1">
<li>Nodes receive an IP address from the Azure virtual network subnet.</li>
<li>Pods receive an IP address from a logically different address space than the nodes’ Azure virtual network subnet.</li>
<li>Network address translation (NAT) is then configured so that the pods can reach resources on the Azure virtual network.</li>
<li>The source IP address of the traffic is translated to the node’s primary IP address.</li>
</ol>
<p>We will refrain from explaining some of the implications of these points: as we go along our step-by-step exploration of how a typical HTTP(S) request, they will become more obvious.</p>
<p>This diagram is a good starting point for the concepts we will explore:</p>
<figure>
<img src="./aks-sas-overview.png" alt="" /><figcaption>SAS Viya on AKS</figcaption>
</figure>
<ul>
<li>SAS Viya, as a solution, is composed of Pods, Services and Ingresses, just a few of the many other types of components (including Deployments, container images, Secrets, etc.): those were chosen given the very direct role they play when describing the network flows.</li>
<li>The NGINX Ingress controller is a Layer 7 proxy and HTTP server, a requirement for SAS Viya due to the way it allows URL-based routing, among other features.</li>
<li>The entire solution is deployed in Microsoft Azure, namely the Azure Kubernetes Service and the Azure Load Balancer.</li>
</ul>
<h1 id="from-the-inside-out">From the inside out</h1>
<p>For our journey, we will use SAS Studio as our starting point: while other components in SAS Viya will have different configurations, SAS Studio is a good example of how most microservices in SAS Viya work.</p>
<p>The following image shows the network configuration of the cluster; there’s no need to fully understand what this means right now, but we will be referring back to this information as we go along.</p>
<figure>
<img src="./aks_network_overview.png" alt="" /><figcaption>AKS network overview</figcaption>
</figure>
<p>We should note that the details above match our earlier description of the <code>kubenet</code> CNI, and in simple terms determines which address space is reserved for specific uses in the cluster.</p>
<p>With that out of the way, let’s start!</p>
<h2 id="microservices-the-pod">Microservices: the Pod</h2>
<figure>
<img src="./pod-view.png" alt="" /><figcaption>The SAS Studio pod</figcaption>
</figure>
<p>The actual application that will interact with our requests - the closest equivalent to what would be the application program in a “traditional” server installation - is running inside a pod, <a href="https://kubernetes.io/docs/concepts/workloads/pods/">the smallest deployable units of computing deployed in Kubernetes</a>,a group of one or more containers that share storage and network resources, and that are always co-scheduled.</p>
<p>SAS Viya is composed of many pods, and we can easily get the one that has the SAS Studio application running:</p>
<pre class="console"><code>$ k get pod|grep sas-studio-app
sas-studio-app-75b59846b4-2s69z                                   1/1     Running      0             37m</code></pre>
<p>Taking a look into that pod, we can see that it’s running very few processes, all of which are directly related with the application: unlike a typical Linux server, there aren’t lots of background processes and other applications running. This is one of the advantages of containers: we have a light-weight Linux server dedicated to SAS Studio, complete with all the necessary dependencies.</p>
<pre class="console"><code>$ k  exec --stdin --tty sas-studio-app-75b59846b4-2s69z -- ps -efw
UID        PID  PPID  C STIME TTY          TIME CMD
sas          1     0  0 Jul31 ?        00:00:01 /usr/local/bin/tini -- /opt/sas/viya/home/bin/sas-studio-app
sas          7     1  0 Jul31 ?        00:00:00 /bin/bash /opt/sas/viya/home/bin/sas-studio-app
sas         50     7  0 Jul31 ?        00:04:26 /usr/lib/jvm/java-11-openjdk-11.0.15.0.9-2.el8_5.x86_64/bin/java --add-opens java.base/java.lang=ALL-UNNAMED -D
sas       1877     0  3 09:04 pts/0    00:00:00 ps -efw</code></pre>
<p>We can also infer that SAS Studio is a Java application: microservices make so-called “polyglot” applications trivial to develop, and SAS Viya is one example of that (apart from Java, there are microservices written in Go, for example).</p>
<p>Back into our SAS Studio application, we can get the IP that it’s listening to easily enough:</p>
<pre class="console"><code>$ k get pod sas-studio-app-75b59846b4-2s69z -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP            NODE                               NOMINATED NODE   READINESS GATES
sas-studio-app-75b59846b4-2s69z   1/1     Running   0          14h   10.244.2.21   aks-stateful-39930745-vmss000005   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>The IP, <code>10.244.2.21</code> is within the Pod CIDR that we seen in the <code>kubenet</code> configuration summary: all pods get an IP from this network segment, and by default they can be contacted using that IP and the right port. In our example, the ports where SAS Studio listens to are 8080 and 10445:</p>
<pre class="console"><code>$ k get pod sas-studio-app-75b59846b4-2s69z  -o jsonpath=&#39;{.spec.containers[*].ports}&#39;
[{&quot;containerPort&quot;:8080,&quot;name&quot;:&quot;http&quot;,&quot;protocol&quot;:&quot;TCP&quot;},{&quot;containerPort&quot;:10445,&quot;name&quot;:&quot;http-internal&quot;,&quot;protocol&quot;:&quot;TCP&quot;}]</code></pre>
<p>We can test port 8080 by using <code>curl</code> within the pod itself, targetting localhost:</p>
<pre class="console"><code>$ k exec --stdin --tty sas-studio-app-75b59846b4-2s69z -- curl -k https://localhost:8080/SASStudio/
{&quot;timestamp&quot;:1659346441951,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;/SASStudio/&quot;}</code></pre>
<p>… and, unsurprisingly, it works (we will not use authorization in this examples to keep things simple: getting the reply is enough to test the network flow).</p>
<p>To test if we can reach the service from outside the pods, we will now test from another one. Using a container image with network tools (<a href="https://github.com/jedrecord/nettools">jedrecord/nettools</a>), we spin up a pod and login interactively, then try to access SAS Studio using the IP and port.</p>
<pre class="console"><code>$ kubectl run -it --rm  --image=jrecord/nettools nettools --restart=Never
If you don&#39;t see a command prompt, try pressing enter.
[root@nettools /]# curl -k https://10.244.2.21:8080/SASStudio/
{&quot;timestamp&quot;:1659349707524,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;/SASStudio/&quot;}[root@nettools /]#
</code></pre>
<p>It also works, so it seems that we just need that pod IP address and port and we are good to go… but this isn’t true!</p>
<p>The reason is that the pod IP is dynamically assigned by Kubernetes, from a range of addresses reserved for Pods in the node. The most direct implication of this is that the IP information should be considered ephemeral, because it actually is: whenever the pod is restarted it will be assigned a different IP.</p>
<p>One typical reason for this would be a restart of the cluster, but since we are not concerned with end-user availability, we can be more blunt and delete the SAS Studio pod we’ve been using until now</p>
<pre class="console"><code>$ k delete pod sas-studio-app-75b59846b4-2s69z
pod &quot;sas-studio-app-75b59846b4-2s69z&quot; deleted</code></pre>
<p>Kubernetes will automatically launch a new one, this being one of the reason why Kubernetes is a container orchestration platform:</p>
<pre class="console"><code>$ k get pod|grep sas-studio-app
sas-studio-app-75b59846b4-gnghj                                   0/1     Running      0             32s</code></pre>
<p>It looks identical, but a closer look will show a different IP address:</p>
<pre class="console"><code>$ k get pod sas-studio-app-75b59846b4-gnghj -o wide
NAME                              READY   STATUS    RESTARTS   AGE     IP             NODE                                NOMINATED NODE   READINESS GATES
sas-studio-app-75b59846b4-gnghj   1/1     Running   0          2m11s   10.244.5.121 aks-stateless-51044827-vmss000004   &lt;none&gt;           &lt;none&gt;</code></pre>
<p>When we redo our connection test, the command that previously worked now… doesn’t:</p>
<pre class="console"><code>root@nettools /]# curl -k https://10.244.2.21:8080/SASStudio/
curl: (7) Failed to connect to 10.244.2.21 port 8080: No route to host</code></pre>
<p>Using the new Pod IP will work:</p>
<pre class="console"><code>[root@nettools /]# curl -k https://10.244.5.121:8080/SASStudio/
{&quot;timestamp&quot;:1659353103484,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;/SASStudio/&quot;}[root@nettools /]#</code></pre>
<p>We have so far determined a couple of important points:</p>
<ul>
<li>SAS Viya has many Pods running, one of which is the one for SAS Studio.</li>
<li>The SAS Studio Pod is based on a container image that runs the Java application (Spring-based).</li>
<li>The pod gets assigned a random IP from the Pod CIDR range when it starts, every time it starts.</li>
<li>We can reach that IP from other pods in the cluster, but we can’t depend on the IP because it changes.</li>
</ul>
<p>It should be noted that it’s not only the IP that is nonpermanent: Pods themselves are considered nonpermanent resoures, and they can (and are) created and destroyed dynamically by a Deployment resource.</p>
<blockquote>
<p><strong>Note</strong> We represented some other resources in the initial diagram: a Secret (that stores certificate tokens for use in the pod), a ConfigMap (that makes certificates available to use in the pod), and a Deployment (the defines all of these in a single logical unit, and through a ReplicaSet makes sure that the correct amount of pods are running at all times); they will not appear in the next diagrams to simplify things and keep the focus on the network flow.</p>
</blockquote>
<p>We need a way to keep track of the IPs so that pods can interact safely. Fortunately, Kubernetes has a way to address this, and much more: <strong>Services</strong>.</p>
<h2 id="internal-load-balancing-the-service">Internal load-balancing : the Service</h2>
<figure>
<img src="./svc-view.png" alt="" /><figcaption>The SAS Studio Service</figcaption>
</figure>
<p>A <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Kubernetes Service</a> is a Kubernetes resource that allows us to define a set of target pods, usually determined by a <em>selector</em>, and that has a stable, cluster-wide IP.</p>
<p>Continuing with our SAS Studio journey, SAS Viya includes a Service for it (and, in general, a Service for all the other components), the details of which include the IP address:</p>
<pre class="console"><code>$ k get service sas-studio-app -o wide
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
sas-studio-app   ClusterIP   10.0.72.50   &lt;none&gt;        443/TCP   3d    app.kubernetes.io/name=sas-studio-app,sas.com/deployment=sas-viya
</code></pre>
<p>This Service has the Cluster IP of 10.0.72.50 - notice that it’s within the Service CIDR of the cluster, different than the one for Pods - and listens in port 443. The selector targets pods that have two labels: one indicating that they are part of SAS Viya, and the other that they are named <code>sas-studio-app</code>.</p>
<p>A closer look into the Service definition will make the connection between it and the pods we have seen in the previous section clearer:</p>
<pre class="console"><code>$ k describe service sas-studio-app
Name:              sas-studio-app
[...]
Selector:          app.kubernetes.io/name=sas-studio-app,sas.com/deployment=sas-viya
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.0.72.50
IPs:               10.0.72.50
Port:              http  443/TCP
TargetPort:        http/TCP
Endpoints:         10.244.5.121:8080
Session Affinity:  None
Events:            &lt;none&gt;</code></pre>
<p>This Service listens on 10.0.72.50:443, and directs traffic to pods that match the selector at port 8080. Looking closely we can find the IP of the SAS Studio pod, from last section, in the <code>Endpoints</code> entry.</p>
<p>Returning to our <code>nettools</code> pod, we can test this flow:</p>
<pre class="console"><code>[root@nettools /]# curl -k https://10.0.72.50/SASStudio/
{&quot;timestamp&quot;:1659353286417,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;</code></pre>
<p>The Service IP of the Service, which can be reached anywhere in the cluster, sends the traffic to the SAS Studio pod as expected; note that we are now using the default HTTPS port since that’s the one where the Service listens to.</p>
<p>One additional feature of Services are the built-in DNS support: while the IP is stable throughout the Service lifecycle, a DNS <code>A</code> entry is created automatically using the form <code>&lt;service-name&gt;.&lt;namespace&gt;.&lt;cluster-name&gt;</code>:</p>
<pre class="console"><code>[root@nettools /]# curl -k https://sas-studio-app.sas-viya.svc.cluster.local/SASStudio/
{&quot;timestamp&quot;:1659353439229,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;/SASStudio/&quot;}</code></pre>
<p>We now have a stable way to make our requests reach the SAS Studio pod. In the case of applications with multiple replicas, the Service will keep track of the health of the endpoints that match the selector and load-balance traffic automatically. Whenever pods get destroyed and rescheduled (for example, a cluster node goes down and pods are automatically restarted in another node), the Service will update its configuration to point to the right place.</p>
<p>In this stretch of our path we have discovered some interesting tidbits:</p>
<ul>
<li>Services redirect and load-balance traffic using a selector.</li>
<li>Services keep track of the health of pods, updating the backend IPs dynamically.</li>
<li>Services provide us with a stable CLuster IP and a cluster internal DNS hostname for the service.</li>
</ul>
<p>With this, we are almost at the end of our journey, but there’s something important missing: Services provide a Cluster IP which is internal, so they are not directly accessable by the outside world. For that, we will need something else: an Ingress.</p>
<h2 id="exposing-to-the-outside-world-the-ingress">Exposing to the outside world: the Ingress</h2>
<figure>
<img src="./ing-view.png" alt="" /><figcaption>The SAS Studio Ingress</figcaption>
</figure>
<p>We went from Pods, to Services, and now we reach another layer: <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">the Ingress</a>. In it’s simpler form, an Ingress manages external access, routing requests coming from the outside to Services inside the cluster.</p>
<p>Just as for Services, SAS Viya deploys a number of Ingresses, including one for SAS Studio:</p>
<pre class="console"><code>$ k get ingress sas-studio-app
NAME             CLASS   HOSTS                                               ADDRESS         PORTS     AGE
sas-studio-app   nginx   doriath.beleriand.cloud,*.doriath.beleriand.cloud   20.103.174.39   80, 443   3d1h</code></pre>
<p>This information makes it clear we are dealing with exposing our resources: the <code>HOSTS</code> field indicates the externally-accessible hostname that we must define when deploying SAS Viya, and it has an <code>ADDRESS</code> field with a public IP address (in this case, sincce it could also be an IP in a private range, but it would indicate the IP associated with the external hostname).</p>
<p>Describing this specific Ingress enables us to get a better understanding on what goes on:</p>
<pre class="console"><code>$ k describe ingress sas-studio-app
Name:             sas-studio-app
Namespace:        sas-viya
Address:          20.103.174.39
Default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
TLS:
  sas-ingress-certificate-24mffmch6d terminates doriath.beleriand.cloud,*.doriath.beleriand.cloud
Rules:
  Host                       Path  Backends
  ----                       ----  --------
  doriath.beleriand.cloud
                             /SASStudio   sas-studio-app:443 (10.244.5.121:8080)
  *.doriath.beleriand.cloud
                             /SASStudio   sas-studio-app:443 (10.244.5.121:8080)
[...]</code></pre>
<p>The above configuration can be read like this: whenever there is a request to <code>https://doriath.beleriand.cloud/SASStudio</code>, as indicated by the <code>Host</code> and <code>Path</code>, traffic should be sent to the <code>sas-studio-app</code> Service, listening on port 443. Note the pod IP address and port between parenthesis: this is additional information that is being displayed, and not a part of the Ingress definition. It’s there to facilitate our view on the active path that requests will take by adding the information of the active Service endpoints.</p>
<p>The Ingress is, ultimately, a rule that determines how traffic will be dealt with, but a rule needs something to implement it: in Kubernetes, this is the Ingress Controller. In SAS Viya one of the requirements is an <a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress controller</a>.</p>
<h2 id="the-nginx-ingress-controller">The NGINX Ingress controller</h2>
<figure>
<img src="./ingc-view.png" alt="" /><figcaption>The NGINX Ingress Controller</figcaption>
</figure>
<p>If the idea of defining rules to direct HTTP traffic seems something that an HTTP server or proxy would do, well, that’s because that’s true: the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a> is a Layer 7 proxy that directs requests to the backend, and specifically the NGINX Ingress controller uses NGINX as a reverse proxy and load-balancer.</p>
<p>As for NGINX itself, it runs in a pod (or more than one, two are a common setting) in its own namespace:</p>
<pre class="console"><code>$ k get pods -n ingress-nginx -o wide
NAME                                        READY   STATUS    RESTARTS   AGE   IP           NODE                             NOMINATED NODE   READINESS GATES
ingress-nginx-controller-84cc585d9c-jgp7g   1/1     Running   0          18h   10.244.3.6   aks-system-16375213-vmss000004   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>The reason we are talking about “Ingress” and “Ingress controller” is mostly because, when NGINX is used in this role in Kubernetes, the way we can configure it is different and completely integrated with the Kubernetes: instead of manually adding rules to the NGINX configuration files, we specify them as Ingress rules and they get automatically added to the configuration, as we can easily see by checking the <code>nginx.conf</code> file in the pod:</p>
<pre class="console"><code>$ k  exec --stdin --tty pod/ingress-nginx-controller-84cc585d9c-jgp7g -n ingress-nginx -- bash
bash-5.1$ grep SASStudio /etc/nginx/nginx.conf
        location ~* &quot;^/SASStudio/&quot; {
            set $location_path  &quot;/SASStudio&quot;;
        location ~* &quot;^/SASStudio&quot; {
            set $location_path  &quot;/SASStudio&quot;;
        location ~* &quot;^/SASStudio/&quot; {
            set $location_path  &quot;/SASStudio&quot;;
        location ~* &quot;^/SASStudio&quot; {
            set $location_path  &quot;/SASStudio&quot;;</code></pre>
<p>Additionally, the NGINX Ingress controller automates other aspects which are needed when working in a Kubernetes context, and there is one aspect which is important to highlight: the way it interact with cloud environments to provide a public endpoint IP.</p>
<h2 id="reaching-the-ingress-cloud-load-balancers">Reaching the Ingress: cloud Load Balancers</h2>
<p>The last step in our journey, this is the first an only which is outside the Kubernetes cluster itself.</p>
<p>We’ve seen that to expose the internal Services, we use an NGINX Ingress controller that will be configured, through Ingress resources, to direct traffic to different Services.</p>
<p>NGINX pods can be deployed in different nodes in the cluster (just like any other pod), so any external connectivity needs to address how to provide a stable external IP address that load-balances all the cluster nodes: we need a single, external endpoint that will send the traffic to the NGINX pod in the cluster.</p>
<p>For <a href="https://kubernetes.github.io/ingress-nginx/deploy/#cloud-deployments">cloud deployment, and specifically for AKS</a>, NGINX automatically creates an external Load Balancer by using a <code>LoadBalancer</code> Service:</p>
<pre class="console"><code>$ k get svc ingress-nginx-controller -n ingress-nginx -o wide
NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE    SELECTOR
ingress-nginx-controller   LoadBalancer   10.0.200.113   20.103.174.39   80:30623/TCP,443:30954/TCP   3d3h   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx</code></pre>
<p>This type of Service should not be mistaken by the <code>ClusterIP</code> types we have already covered: a Load Balancer Service will interact with the Cloud infrastructure and automate the creation of an external load balancer, and will store the IP that is returned. As you can see, the <code>EXTERNAL-IP</code> of this Service is the same that appears in all Ingress rules, and is indicated in the <code>LoadBalancer Ingress</code> field as well:</p>
<pre class="console"><code>$ k describe svc ingress-nginx-controller -n ingress-nginx
Name:                        ingress-nginx-controller
Namespace:                   ingress-nginx
[...]
Selector:                    app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
Type:                        LoadBalancer
IP Family Policy:            SingleStack
IP Families:                 IPv4
IP:                          10.0.200.113
IPs:                         10.0.200.113
LoadBalancer Ingress:        20.103.174.39
Port:                        http  80/TCP
TargetPort:                  http/TCP
NodePort:                    http  30623/TCP
Endpoints:                   10.244.3.6:80
Port:                        https  443/TCP
TargetPort:                  https/TCP
NodePort:                    https  30954/TCP
Endpoints:                   10.244.3.6:443
Session Affinity:            None
External Traffic Policy:     Local
HealthCheck NodePort:        31081
[...]</code></pre>
<p>Just like any Service, it has a selector, which in this case directs traffic to the NGINX Ingress controller pods (the <code>Endpoints</code> field has the IP of the NGINX pod from last section). The internal IP, 10.0.200.113, can also be used to send traffic to NGINX, but obviously will only work inside the cluster.</p>
<p>In terms of Azure, the topology will be something like this:</p>
<figure>
<img src="./azure-lb.png" alt="" /><figcaption>Azure Load Balancer</figcaption>
</figure>
<p>The cloud load balancer will check the status of all nodes by connecting to http://<node-external-ip>:31081/healthz (this can be seen in the <em>Health Probes</em> blade in Azure). The nodes where the ingress pod is running will answer with <code>HTTP 200</code>, and traffic will be sent to that node’s external IP on the ports where it’s listening - 30623(HTTP) and 30954 (HTTPS).</p>
<blockquote>
<p><strong>Note</strong> there is a subtle detail that we are not showing in our diagrams or descriptions: when looking into the Azure Load Balancer configuration in AKS, the target port for the backend is set at 80 (HTTP) and 443 (HTTPS)… but the LoadBalancer Service definition creates NodePorts, in each node, listening on 30623(HTTP) and 30954 (HTTPS). This is a <em>very</em> lightly documented feature of AKS: what happens is that in the Kubernetes nodes, at the host Linux level, there are <code>iptables</code> rules that do a DNAT on those ports, when they have the load balancer IP as the target: in short, they transparently redirect traffic coming from the load balancer to the Service ports. This is very hard to test and can only be detected by looking into the <code>iptables</code> chains at the host level (check https://github.com/kubernetes/kubernetes/issues/58759 for more details).</p>
</blockquote>
<p>This load balancer IP is the final destination of our analysis: this is what we will use in our browser to access SAS Viya, in general, and SAS Studio in particular. We are now ready to make our journey back to the pod, now with a clearer understanding of what it entails.</p>
<h1 id="sas-viya-networking-or-there-and-back-again">SAS Viya networking, Or, There and Back Again</h1>
<p>Having gone through this step-by-step exploration of the different components, we should now be more prepared for a diagram that goes a bit more in-depth, and that we can use as a reference:</p>
<figure>
<img src="./sas-viya-k8s.png" alt="" /><figcaption>SAS Viya on AKS</figcaption>
</figure>
<p>Starting from the outside - for example, an end-user browser trying to access SAS Studio - this is how the requests get into the SAS Studio pod:</p>
<ol type="1">
<li>Someone makes a request to https://doriath.beleriand.cloud/SASStudio . This hostname was determined from the start as the one for SAS Viya.</li>
<li>The hostname is converted into an IP address, almost always through DNS (but could be through <code>/etc/hosts</code> as well).</li>
<li>This IP address, <code>20.103.174.39</code>, is the IP of the frontend of Azure Load Balancer, which has rules to direct traffic on ports 80 and 443 towards all nodes of the Kubernetes cluster. This Load Balancer was created because of the <code>LoadBalancer</code> Service that was added by the NGINX Ingress controller, which interacted with Azure to spin up the cloud load balancer, and stored the IP that was configured as the frontend.</li>
<li>The load balancer determines which nodes in the cluster are able to receive the request, using a health probe, and send the request to the port defined in the LoadBalancer Service on that node’s external IP.</li>
<li>The request arrives at the cluster, and is sent to an NGINX pod.</li>
<li>The path <code>/SASStudio</code> is processed as per the NGINX rules, which are determined by Ingress resources: in this case, there is an Ingress definition that says that traffic with that path component should be sent to the <code>sas-studio-app</code> Service, on port 443.</li>
<li>The <code>sas-studio-app</code> Service receives the request and sends it to the configured endpoints, which are the pods that fit a specific selector. In this case, the <code>sas-studio-app</code> pod, in whatever internal IP it currently has, on port 8080.</li>
<li>The <code>sas-studio-app</code> pod (named something like <code>sas-studio-app-75b59846b4-gnghj</code>) receives the request on its IP, port 8080, where the SAS Studio application is running.</li>
</ol>
<p>After receiving the request, the SAS Studio application sends the reply: note that this is not up the same circuit, and will, in the most simple cases, be sent through the external IP of the node where the pod is running, which is possibly then directed to an outgoing IP in the same cloud load balancer (this is the situation in the AKS cluster used here).</p>
<h1 id="final-words">Final words</h1>
<p>TBD</p>
<h1 id="possible-additions">Possible additions</h1>
<ul>
<li>A reference to SSL termination in the Ingress.</li>
<li>A reference to Service Meshes</li>
</ul>

</body>
</html>
