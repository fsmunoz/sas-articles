<h1 id="introdution">Introdution</h1>
<p>SAS Viya has adopted Kubernetes as the underlying target environment, opening up immense possibilities for all that wish to combine the industry-leading abilities of SAS in analytics, data science and AI, with a cloud-native deployment approach that aligns with current strategic roadmaps, not the least because of the potential benefits it brings to the table: integration with CI/CD for deployment and code, enabling hybrid cloud scenarios by supporting both on-premises and cloud deployments, multi-cloud support by default, and perhaps more importantly, a product development process that is more agile, with more releases, a faster incorporation of new features.</p>
<p>If the move towards Kubernetes seems obvious today (especially considering that Kubernetes has become the industry standard in container orchestration, and is the underlying platform on which a lot of other extremely popular technologies are built on), anyone coming from a SAS 9 background can find the target architecture somewhat hermetic: instead of a number of servers with a specific set of well-known components, SAS Viya depends on nodes, deployments, services, ingresses, and pods, plus many other concepts that can seem complex - and especially so in terms of the network aspects.</p>
<p>This is precisely the aspect we will address in this article: how do network requests work in a Kubernetes context, starting from a single pod and working our way out of the Kubernetes onion, peeling off the apparent complexity layer by layer - and once we are comfortably on the outside, trace our way back, now with a clearer understanding of how SAS Viya will work.</p>
<h2 id="kubernetes-an-overview-of-the-network-model">Kubernetes: an overview of the network model</h2>
<p><overview of the k8s network model></p>
<h2 id="sas-viya-and-the-evolution-towards-microservices">SAS Viya and the evolution towards microservices</h2>
<p>&lt;quick view of the SAS 9 &gt; SAS Viya 3.5 &gt; SAS Viya evolution&gt;</p>
<h1 id="from-the-inside-out">From the inside out</h1>
<p>For our journey, we will use SAS Studio as our starting point: while other components in SAS Viya will have different configurations, SAS Studio is a good example of how most microservices in SAS Viya work.</p>
<p>Our environment will be</p>
<ul>
<li>A Kubernetes cluster on Microsoft Azure (Azure Kubernetes Service)</li>
<li>SAS Viya 2022.1.3, deployed in a <code>sas-viya</code> namespace (configured to be the active Kubernetes context)</li>
</ul>
<p>Given that we are describing standard Kubernetes concepts, the vast majority of this article will apply to all Kubernetes and SAS Viya versions.</p>
<p>The network configuration of the cluster is as follows: there’s no need to understand what this means right now, but we will be referring back to this information as we go along</p>
<figure>
<img src="./aks_network_overview.png" alt="" /><figcaption>AKS network overview</figcaption>
</figure>
<p>With that out of the way, let’s start!</p>
<h2 id="microservices-the-pod">Microservices: the Pod</h2>
<p>The actual application that will interact with our requests - the closest equivalent to what would be the application program in a “traditional” server installation - is running inside a pod, <a href="https://kubernetes.io/docs/concepts/workloads/pods/">the smallest deployable units of computing deployed in Kubernetes</a>,a group of one or more containers that share storage and network resources, and that are always co-scheduled.</p>
<p>SAS Viya is composed of many pods, and we can easily get the one that has the SAS Studio application running:</p>
<pre><code>$ k get pod|grep sas-studio-app
sas-studio-app-75b59846b4-2s69z                                   1/1     Running      0             37m</code></pre>
<p>Taking a look into that pod, we can see that it’s running very few processes, all of which directly related with the application: unlike a typical Linux server, there aren’t lots of background processes and other applications running. This is one of the advantages of containers: we have a light-weight Linux server dedicated to SAS Studio, complete with all the necessary dependencies.</p>
<pre><code>$ k  exec --stdin --tty sas-studio-app-75b59846b4-2s69z -- ps -efw
UID        PID  PPID  C STIME TTY          TIME CMD
sas          1     0  0 Jul31 ?        00:00:01 /usr/local/bin/tini -- /opt/sas/viya/home/bin/sas-studio-app
sas          7     1  0 Jul31 ?        00:00:00 /bin/bash /opt/sas/viya/home/bin/sas-studio-app
sas         50     7  0 Jul31 ?        00:04:26 /usr/lib/jvm/java-11-openjdk-11.0.15.0.9-2.el8_5.x86_64/bin/java --add-opens java.base/java.lang=ALL-UNNAMED -D
sas       1877     0  3 09:04 pts/0    00:00:00 ps -efw</code></pre>
<p>We can also infer that SAS Studio is a Java application: microservices make so-called “polyglot” applications trivial to develop, and SAS Viya is one example of that (apart from Java, there are microservices written in Go, for example).</p>
<p>Back into our SAS Studio application, we can get the IP that it’s listening to easily enough:</p>
<pre><code>k get pod sas-studio-app-75b59846b4-2s69z -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP            NODE                               NOMINATED NODE   READINESS GATES
sas-studio-app-75b59846b4-2s69z   1/1     Running   0          14h   10.244.2.21   aks-stateful-39930745-vmss000005   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>The IP is within the Pod CIDR: all pods get an IP from this network segment, and by default they can be contacted using that IP and the right port. In our example, the ports where SAS Studio listens to are 8080 and 10445:</p>
<pre><code>$ k get pod sas-studio-app-75b59846b4-2s69z  -o jsonpath=&#39;{.spec.containers[*].ports}&#39;
[{&quot;containerPort&quot;:8080,&quot;name&quot;:&quot;http&quot;,&quot;protocol&quot;:&quot;TCP&quot;},{&quot;containerPort&quot;:10445,&quot;name&quot;:&quot;http-internal&quot;,&quot;protocol&quot;:&quot;TCP&quot;}]</code></pre>
<p>We can test port 8080 by using <code>curl</code> within the pod itself, targetting localhost:</p>
<pre><code>$ k exec --stdin --tty sas-studio-app-75b59846b4-2s69z -- curl -k https://localhost:8080/SASStudio/
{&quot;timestamp&quot;:1659346441951,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;/SASStudio/&quot;}</code></pre>
<p>… and, unsurprisingly, it works (we will not use authorization in this examples to keep things simple: getting the reply is enough to test the network flow).</p>
<p>To test if we can reach the service from outside the pods, we will now test from another one. Using a container image with network tools (<a href="https://github.com/jedrecord/nettools">jedrecord/nettools</a>), we spin up a pod and login interactively, then try to access SAS Studio using the IP and port.</p>
<pre><code>$ kubectl run -it --rm  --image=jrecord/nettools nettools --restart=Never
If you don&#39;t see a command prompt, try pressing enter.
[root@nettools /]# curl -k https://10.244.2.21:8080/SASStudio/
{&quot;timestamp&quot;:1659349707524,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;&quot;,&quot;path&quot;:&quot;/SASStudio/&quot;}[root@nettools /]#
</code></pre>
<p>It also works, so it seems that we just need that pod IP address and port and we are good to go… but this isn’t true. The reason is that the pod IP is dynamically assigned by Kubernetes, from a range of addresses reserved for Pods in the node. The most direct implication of this is that the IP information should be considered ephemeral, because it actually is: whenever the pod is restarted it will be assigned a different IP.</p>
<p>One typical reason for this would be a restart of the cluster, but since we are not concerned with end-user availability, we can be more blunt and delete the SAS Studio pod we’ve been using until now</p>
<pre><code>$ k delete pod sas-studio-app-75b59846b4-2s69z
pod &quot;sas-studio-app-75b59846b4-2s69z&quot; deleted</code></pre>
<p>Kubernetes will automatically launch a new one, this being one of the reason why Kubernetes is a container orchestration platform:</p>
<pre><code>$ k get pod|grep sas-studio-app
sas-studio-app-75b59846b4-gnghj                                   0/1     Running      0             32s</code></pre>
<p>It looks identical, but a closer look will show a different IP address:</p>
<pre><code>$ k get pod sas-studio-app-75b59846b4-gnghj -o wide
NAME                              READY   STATUS    RESTARTS   AGE     IP             NODE                                NOMINATED NODE   READINESS GATES
sas-studio-app-75b59846b4-gnghj   1/1     Running   0          2m11s   10.244.5.121 aks-stateless-51044827-vmss000004   &lt;none&gt;           &lt;none&gt;</code></pre>
<p>When we redo our connection test, the command that previously worked now… doesn’t:</p>
<pre><code>root@nettools /]# curl -k https://10.244.2.21:8080/SASStudio/
curl: (7) Failed to connect to 10.244.2.21 port 8080: No route to host</code></pre>
<p>Using the new Pod IP will work:</p>
<pre><code>[root@nettools /]# curl -k https://10.244.5.21:8080/SASStudio/
{&quot;version&quot;:2,&quot;httpStatusCode&quot;:401,&quot;errorCode&quot;:401,&quot;message&quot;:&quot;Full authentication is required to access this resource&quot;,&quot;details&quot;:[&quot;path: /SASStudio/&quot;,&quot;correlator: dac6d5b5-0f9f-4c40-9c40-31c826320026&quot;]}[root@nettools /]#</code></pre>
<p>We have so far determined a couple of important points:</p>
<ul>
<li>SAS Viya has many Pods running, one of which is the one for SAS Studio.</li>
<li>The SAS Studio Pod is based on a container image that runs the Java application (Spring-based).</li>
<li>The pod gets assigned a random IP from the Pod CIDR range when it starts, every time it starts.</li>
<li>We can reach that IP from other pods in the cluster, but we can’t depend on the IP because it changes.</li>
</ul>
<p>Fortunately, Kubernetes has a smart way to</p>
<h2 id="internal-load-balancing-the-service">Internal load-balancing : the Service</h2>
<h2 id="exposing-to-the-outside-world-the-ingress">Exposing to the outside world: the Ingress</h2>
<h2 id="a-proxy-by-any-other-name-the-nginx-ingress-controller">A proxy by any other name: the NGINX Ingress controller</h2>
<h2 id="dealing-with-kubernetes-high-availability-load-balancers">Dealing with Kubernetes high-availability: Load Balancers</h2>
<h1 id="from-the-outside-in">From the outside in</h1>
<h2 id="dns-and-the-configured-ingress-name">DNS and the configured Ingress name</h2>
<h2 id="reaching-the-load-balancer">Reaching the Load Balancer</h2>
<h2 id="section"></h2>
<h2 id="section-1"></h2>
